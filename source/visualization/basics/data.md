# 数据
<!-- :label:`visualization-data` -->

可视化的处理对象是各种类型的数据。
数据是可视化的原材料，深入理解数据的特点与结构是实现有效可视化的基础。
本节将从数据的定义与分类、数据的获取与预处理、数据可视化的挑战三个方面，系统地阐述可视化中的数据基础知识。

## 什么是数据

### 数据的定义与特征
数据是客观世界的抽象记录，是信息表达的载体，也是可视化的核心原料。其本质是通过符号、数字、文字、图像等形式对现实世界的观察、测量或计算结果进行编码。
数据的特征可归纳为以下几点：

- 客观性：数据反映客观事实，但需通过主观解释赋予意义。例如，温度传感器记录的“25℃”是数据，结合时间、地点后成为“北京夏季平均气温”的信息。
- 多维度性：数据可包含多个属性维度。例如，一次电商交易可能涉及用户ID、商品类别、价格、时间、地理位置等多个字段。
- 动态性：数据随时间和环境变化。实时数据流（如股票行情、社交媒体话题）需要动态可视化以捕捉瞬时状态

广义上，数据可以使用 "6W" 模型来描述，即 What(数据是什么)、When(数据的时间属性)、Where(数据的空间属性)、Who(数据的对象)、Why(数据产生的原因)、How(数据产生的方式)。
这些属性构成了对数据多方面特性的描述。

### 数据分类
数据分类是理解其特性和选择可视化方法的基础，常见的分类维度包括：

**1. 按数据结构：**

- 结构化数据（Structured Data）：
结构化数据具有固定的格式和预定义的数据模型，通常以表格的形式存储在关系型数据库中。每个数据项都有明确的字段和数据类型，不同数据项之间的关系也有明确定义。结构化数据的特点是高度组织化，易于查询、分析和处理。
常见的结构化数据包括：
  - 关系型数据库中的数据表，如 MySQL、Oracle、SQL Server 等。
  - 电子表格中的数据，如 Excel、CSV 文件等。
  - 具有固定格式的日志文件，如 Web 服务器日志、应用程序日志等。

- 半结构化数据（Semi-structured Data）
半结构化数据介于完全结构化和非结构化之间，具有一定的结构和组织，但不如结构化数据严格。半结构化数据通常以树形结构或图形结构来表示，数据项之间的关系相对松散。常见的半结构化数据格式包括 XML、JSON 等。
常见的半结构化数据包括：
  - XML 文档，如 Web 服务的响应、配置文件等。
  - JSON 数据，如 Web API 的响应、NoSQL 数据库（如 MongoDB）中的文档等。
  - 电子邮件，包含结构化的头部信息和非结构化的正文内容。
  - HTML 文档，包含结构化的标签和非结构化的文本内容。

- 非结构化数据（Unstructured Data）：
非结构化数据没有预定义的数据模型或组织结构，通常以自由格式存储。非结构化数据的特点是多样性和不可预测性，难以用传统的数据库技术直接处理和分析。非结构化数据占据了当今数据的绝大部分。
常见的非结构化数据包括：
  - 文本文档，如 Word 文档、PDF 文件、电子邮件正文等。
  - 图像和图形文件，如 JPEG、PNG、BMP 等。
  - 音频和视频文件，如 MP3、WAV、MP4、AVI 等。
  - 社交媒体数据，如推文、帖子、评论等。
  - 传感器数据，如物联网设备采集的原始数据。

**2. 按数据测量尺度：**

- 定量数据（Quantitative Data）：
定量数据以数值形式表示，具有明确的数学意义，支持算术运算（如加减乘除）。其测量尺度为定距尺度（Interval）或定比尺度（Ratio），能够反映事物的数量、强度或程度差异。
定量数据一般包括：
  - 连续型定量数据：取值在区间内无限细分（如温度、身高）。
  - 离散型定量数据：取值为有限或可数无限个整数（如人口数量、订单数）。

- 定序数据（Ordinal Data）：
定序数据表示类别之间的顺序或等级关系，但无法量化差异程度。其测量尺度为顺序尺度（Ordinal），仅支持排序（如A > B > C），不满足加减运算。
定序数据一般包括：
  - 主观评价数据：如用户满意度评分，疼痛等级等。
  - 等级分类数据：如教育程度，产品评级等。
  - 阶段划分数据：如癌症分期， 项目进度等。

- 定类数据（Nominal Data）：
定类数据表示无顺序或等级差异的纯分类，仅用于标识事物类别。其测量尺度为名义尺度（Nominal），仅支持频次统计，无法进行排序或数学运算。
定类数据一般包括：
  - 自然类别数据：人口属性如性别、血型，地理分类如国家、省份等。
  - 人工编码数据：产品类型如手机、笔记本电脑、平板电脑等，行业分类如金融、教育、医疗、制造业等。
  - 二元数据：是否类数据如用户是否订阅服务（是/否）。
  - 
**3. 按数据属性：**

- 数值型数据（Quantitative Data）：
数值型数据以数字形式表示，支持加减、平均、方差等数学运算和统计分析，并适合通过位置、长度、面积等视觉通道表达数值差异。
常见的数值型数据包括：
  - 连续型数据：取值在某一区间内无限细分的数据，如气象站记录的每小时温度、患者血压值，股价等。
  - 离散型数据：取值为有限或可数无限个整数的数据，如网站点击次数、电商每日订单数、城市人口数量等。

- 类别型数据（Categorical Data）：
类别型数据表示事物的分类或标签，无法直接进行数学运算，类别数量通常固定（如性别、产品类型）。
常见的类别型数据分为：
  - 有序型数据：类别间存在逻辑顺序或等级关系，如教育程度（小学<中学<大学）、用户满意度评分（1~5星）等。
  - 无序型数据：类别间无明确顺序，仅表示分类差异，如用户性别（男/女/其他）、产品颜色（红/蓝/绿）等。

- 时空型数据（Spatiotemporal Data）：
时空型数据包含时间或空间维度信息，反映现象在时空坐标系中的演化规律、分布特征及相互作用关系。具有时空动态性，数据值随时间推移呈现趋势性、周期性或突变性变化，数据分布则随地理位置呈现聚集、扩散或梯度差异。
常见的时空性数据包括：
  - 时间序列数据：按时间戳记录的连续观测值数据，如股票每日收盘价、气象站每小时降雨量等。
  - 地理空间数据：包含地理位置信息（经纬度、行政区划）的数据，如城市GDP（按省份分布）、共享单车骑行轨迹（GPS坐标）。



数据的分类是构建有效分析框架的基石，不同的分类方式服务于不同的分析目标。
数据分类不仅可以界定技术工具的选择边界（如关系型数据库适配结构化数据，自然语言处理应对非结构化文本），也可以规避方法论误用的风险（如对定类数据计算均值）。
理解分类的本质，是从数据混沌中提取信息、从信息中凝练知识的关键跃迁，才能在可视化时 根据数据的不同特点来采取不同的处理与映射方式。


## 数据获取与处理

数据获取与处理是可视化流程的基石，直接影响最终结果的可信度与表达力。本节将系统阐述传统可视化中数据获取的途径、方法及处理流程。

### 数据获取途径与方法
可视化研究的数据获取主要依赖以下途径：

**1. 实验观测数据**

实验观测数据通过受控环境下的物理设备或人工记录获取，强调数据的精确性与可重复性。
例如，气象站通过温度、湿度传感器记录实时气候数据，实验室通过显微镜拍摄细胞分裂过程生成时序图像序列。
此类数据通常用于科学研究（如天体光谱分析、材料力学性能测试），其质量高度依赖仪器精度与实验设计，但可能存在采样频率受限、设备误差累积等问题。

**2. 公开数据集**

公开数据集由学术机构、政府或企业主动开放，为跨领域研究提供标准化数据源。
典型代表包括UCI Machine Learning Repository中的经典机器学习数据集（如鸢尾花分类数据）、政府开放平台（如美国国家海洋和大气管理局发布的全球气温记录）以及行业数据门户（如谷歌公共数据探索器的经济指标）。
此类数据通常经过初步清洗与标注，适合快速验证可视化算法或进行教育演示，但可能因数据更新滞后或字段解释不完整影响深度分析。

**3. 模拟生成数据**

模拟生成数据基于数学模型或计算机仿真构建，用于填补真实数据缺失或探索假设性场景。
例如，计算流体力学（CFD）软件模拟飞机翼型的气流分布，生成压力场与速度场数据；分形算法生成虚拟地形高程模型以测试三维可视化性能。此类数据可控性强，能够生成极端条件（如超高温、超高压）下的理想化数据，但需要警惕模型假设与真实世界的偏差导致结论失真。

**4. 日志与业务系统数据**

日志与业务系统数据从软件操作、用户行为或生产流程中自动记录，反映实际业务状态与用户交互模式。
典型场景包括Web服务器日志（记录页面访问量与用户IP）、电商平台的交易流水（包含商品ID、价格、购买时间）以及工业传感器网络采集的设备运行参数（如转速、温度）。
此类数据具有高时效性与真实性，但常伴随噪声（如爬虫流量）、数据碎片化（多系统孤岛）及隐私合规风险，需通过清洗、关联与脱敏处理方可使用。

### 数据处理

**1. 数据清洗**

数据清洗旨在消除噪声、填补缺失、纠正错误，确保数据质量以适配后续分析与可视化需求。
数据清洗往往使用以下技术：
- 缺失值处理：
  - 删除：直接移除包含缺失值的记录或字段，适用于缺失比例较低（如<5%）且缺失机制完全随机（MCAR）的场景。优点是操作简单且避免引入偏差，缺点是可能损失有用信息。 
  - 填充：通过均值、中位数、众数（类别数据）或插值法（如线性插值、KNN插补）填补缺失值。适用于缺失比例较高或非随机缺失（MNAR）的情况，需警惕填充方法对统计分布的扭曲。
- 异常值检测：
  - 统计方法：Z-score（绝对值>3视为异常）、IQR（四分位距法，超出1.5倍IQR范围）。 
  - 聚类方法：基于密度聚类（如DBSCAN）隔离离群点。
- 去噪：
  - 滤波算法：移动平均（平滑时序数据）、中值滤波（处理图像椒盐噪声）。
  - 平滑处理：Savitzky-Golay滤波器保留信号趋势特征。

**2. 数据转换**

数据转换通过数学运算与结构重组，将原始数据转化为适合分析与可视化的形式。
数据转换常用技术包括：
- 归一化与标准化：
  - 归一化：将数据缩放到[0,1]区间（公式：$ x' = \frac{x - \min(X)}{\max(X) - \min(X)} $），直接移除包含缺失值的记录或字段，适用于量纲差异大的特征（如身高与体重）。 
  - 标准化：转化为均值为0、标准差为1的分布（公式：$ x' = \frac{x - \mu}{\sigma} $），适用于服从正态分布的数据。
- 离散化：
  - 分箱：等宽分箱（固定区间宽度，易受异常值影响）、等频分箱（每箱样本数相同，保持分布均衡）。
  - 聚类分箱：通过K-means聚类划分数据区间，适应非线性分布。
- 特征工程：
  - 衍生变量：从时间戳提取“星期几”“季节”，从地址提取“城市”“邮编”。
  - 降维：主成分分析（PCA）保留最大方差方向，t-SNE优化高维数据可视化聚类。

**3. 数据融合**

数据融合整合多源异构数据，构建统一视图以支持跨维度分析。
数据融合关键技术包括：
- 数据对齐：
  - 时间戳对齐：将不同采样频率的数据通过线性插值或重采样统一到相同时间轴。
  - 空间坐标匹配：地理数据投影转换（如WGS84经纬度转Web墨卡托）。
- 实体解析：
  - 记录链接：基于编辑距离（Levenshtein）或Jaccard相似度消除重复条目。
  - 主数据管理：建立全局唯一标识符（如用户ID）关联分散数据源。
- 关联分析：
  - 连接操作：SQL JOIN按键关联表，图数据库通过边关系查询节点属性。

**4. 数据聚合**

数据聚合通过汇总降低数据复杂度，提升可视化性能与可解释性。
数据聚合主要技术包括：
- 时间窗口聚合：按小时、日、月统计指标（如销售额、用户活跃数）。 
- 空间区域聚合：按行政区划（省、市）或自定义地理网格汇总数据。 
- 分层聚合：OLAP中的上卷（Roll-up）操作，从细粒度（城市）逐步汇总至粗粒度（国家）。

在数据处理流程中，通过数据清洗确保质量，数据转换优化形态，数据融合打通孤岛，数据聚合简化复杂度，这四步流程层层递进，帮助研究人员将原始数据转化为可视化可用的高质量输入。


## 数据科学的挑战

### 数据科学的核心挑战

数据科学的核心挑战不仅源于数据本身，更与可视化目标（如信息传递效率、认知负载平衡）及技术实现约束（如实时渲染性能、交互响应速度）紧密关联。

**1. 数据质量与预处理瓶颈**

- 高维稀疏性：高维数据（如基因表达数据、社交媒体网络）导致可视化中的“维度灾难”，传统降维方法（如PCA）可能丢失关键模式。
- 非结构化数据转化：将文本、图像等非结构化数据转化为可视化友好的结构化形式，需平衡信息损失与表达效率。例如，自然语言处理（NLP）提取关键词可能忽略上下文语义。
- 缺失与噪声处理：数据缺失（如卫星遥感云层遮挡）与噪声（如传感器漂移）直接影响可视化结果的准确性，需设计鲁棒插补与滤波算法。

**2. 实时性与交互性需求**

- 流数据可视化：实时数据流（如股票行情、交通监控）要求可视化系统支持动态更新，传统静态图表（如折线图）难以实现毫秒级刷新。
- 大规模数据渲染：亿级数据点的实时渲染受限于GPU内存与计算能力，需采用层次细节（LOD）或采样策略（如概率像素化）。
- 交互延迟优化：复杂交互（如多视图联动、三维旋转）对计算资源的高消耗可能导致卡顿，需结合WebGL加速与异步计算。

**3. 可扩展性与异构环境适配**

- 跨平台兼容性：可视化工具需适配不同设备（桌面端、移动端、AR/VR头盔），但分辨率、算力差异导致设计复杂度陡增。
- 云边协同架构：边缘设备（如手机、IoT传感器）的计算能力有限，需在本地预处理与云端深度分析间分配任务。例如，移动端实时生成热力图，云端执行聚类分析。
- 多源数据融合：整合数据库、API、文件系统等异构数据源时，Schema映射与语义一致性维护困难，影响可视化结果的逻辑统一性。

**4. 可解释性与用户认知匹配**

- 视觉编码误导：不当的颜色映射（如非线性色阶）或图形选择（如3D饼图透视扭曲）可能引发误读，需遵循格式塔原则与感知心理学规律。
- 认知负载控制：过度复杂的可视化（如多层级网络图）超出用户信息处理能力，需通过渐进式披露（Progressive Disclosure）简化信息层级。
- 领域知识嵌入：通用可视化工具难以满足垂直领域（如地质勘探、医疗影像）的专业需求，需定制领域特定视觉隐喻（如等值线、体绘制）。

**5. 伦理与隐私保护**

- 敏感数据泄露：地理轨迹、医疗记录等敏感信息的可视化可能暴露个人隐私，需采用k-匿名化、差分隐私技术对输出结果脱敏。 
- 算法偏见可视化：可视化模型决策依据（如特征重要性图）时，可能放大数据中的固有偏见（如种族、性别偏差），需结合公平性指标校正。
- 可视化武器化风险：恶意利用可视化误导公众（如扭曲坐标轴夸大差异），需建立可视化伦理规范与审计机制。




 ### 应对挑战的技术演进

为应对上述挑战，研究人员在数据科学领域对方法论与工具链进行了持续创新：

**1. 数据质量增强技术**

- 自动化数据清洗：基于生成对抗网络（GAN）合成高质量训练数据，缓解数据稀缺问题。例如，利用CycleGAN将低分辨率医学影像增强为高分辨率。
- 偏见检测与矫正：通过公平性指标（如统计均等性、机会均等性）量化数据偏见，采用重加权（Reweighting）或对抗训练减少模型偏差。

**1. 高性能计算架构**

- 分布式计算框架：Apache Spark、Dask支持内存计算与任务并行化，加速大规模数据处理。
- 边缘-云协同：在边缘设备执行实时预处理，云端进行深度分析。例如，无人机在边缘端实时检测目标，云端聚合多机数据生成全局态势图。

**3. 可解释AI与合规技术**

- 模型可解释性工具：SHAP（Shapley Additive Explanations）、LIME（Local Interpretable Model-agnostic Explanations）可视化模型决策依据。
- 隐私增强技术：同态加密（Homomorphic Encryption）实现数据在加密状态下计算，平衡隐私保护与数据价值挖掘。


总的来说，可视化所面临的数据科学挑战，本质是数据复杂性、技术约束与人因需求的三重博弈。
解决这些挑战需跨学科协作——数据工程优化预处理流程，计算机图形学突破渲染性能瓶颈，认知科学指导视觉设计，伦理学约束技术边界。
未来研究应聚焦自动化预处理、自适应可视化与负责任设计，在效率、精度与伦理间寻求平衡。
